{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlockMTL Extension Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You work for an e-commerce company that sells consumer electronics. You have a treasure trove of customer reviews, and your goal is to:\n",
    "\n",
    "#### 1. Analyze the sentiment of each review.\n",
    "#### 2. Identify high-impact reviews that require urgent attention.\n",
    "#### 3. Extract recurring themes or topics to understand common issues and areas for product improvement.\n",
    "#### 4. Track customer satisfaction trends over time.\n",
    "\n",
    "### Let us see how to build SQL queries that use `FlockMTL` to achieve all of this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 : Create your OPENAI_API env file\n",
    "\n",
    "Create a file called .env in the current working directory, with the following text:\n",
    "\n",
    "`export OPENAI_API_KEY='<your OPENAI API KEY'`\n",
    "\n",
    "Replace the placeholder with your API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 : Load your OPENAI_API env file\n",
    "\n",
    "We use the dotenv python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./.large_flock/lib/python3.12/site-packages (1.0.1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Install and import DuckDB\n",
    "\n",
    "We are using DuckDB version 1.1.1 for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install duckdb==1.1.1\n",
    "import duckdb\n",
    "print(duckdb.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything goes well, you should see the version being printed, its 1.1.1 for our example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Create a database for our use\n",
    "\n",
    "1. We create a new persistent database connection for our example called `mydb.db`.\\\n",
    "2. We also pass a config dict `config={'allow_unsigned_extensions' : 'true'}` to allow us to load the extension.\n",
    "3. We also load a CSV file called `product_reviews.csv` into a table called `product_reviews`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection to an in-memory database\n",
    "con = duckdb.connect('mydb.db', config={'allow_unsigned_extensions' : 'true'})\n",
    "\n",
    "csv_path = 'product_reviews.csv' #add your own path\n",
    "con.execute(f\"CREATE TABLE product_reviews AS SELECT * FROM read_csv_auto('{csv_path}')\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Load our extension\n",
    "\n",
    "\n",
    "We use the DuckDB `INSTALL` and `LOAD` commands to load our extension\\\n",
    "Set the `path` variable to the location where the extension binary is downloaded, and then execute the following\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x77b02c14eef0>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/home/sunny/work/large_flock_extension/large_flock.duckdb_extension' # replace your path here\n",
    "con.execute(f\"INSTALL '{path}'\")\n",
    "con.execute(\"LOAD large_flock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And that's it ! Our extension is loaded, now we can use the awesome LLM capabilities inside for some fun task!\n",
    "You can go through the available prompts and models we have by executing the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('summarize', 'Summarize the following text: {{text}}'), ('validate_email', 'validate this email: {{email}}'), ('test', 'is this email valid {{email}} the output should be a bool value'), ('check john', 'return the values with the first_name John'), ('keywords', 'extract the keywords from the next text'), ('is_high_impact_review_prompt', 'Determine if the given review is a high-impact review that provides valuable insights. Consider the following factors:\\n\\n1. Sentiment: {sentiment}\\n2. Star Rating: {rating}\\n3. Review Length: {review_length}\\n\\nA high-impact review typically has:\\n\\n- A strong sentiment (very positive or very negative)\\n- An extreme rating (1-2 or 4-5 stars)\\n- Sufficient length to provide detailed feedback (usually more than 50 words)\\n\\nOutput your decision as a boolean (true for high-impact, false for low-impact) with a brief explanation.'), ('sentiment_analysis_prompt', 'Analyze the sentiment of the following product review. Consider both the review text and the star rating. Provide a brief sentiment label (positive, negative, or neutral) and a short explanation for your decision.\\n\\nReview: {review}\\nStar Rating: {rating}\\n\\nOutput your response in the following JSON format:\\n{\\n\"sentiment\": \"positive/negative/neutral\",\\n\"explanation\": \"Brief explanation of the sentiment analysis\"\\n}'), ('prompt_name', 'your_prompt'), ('extract_themes_prompt', '\\nAnalyze the following sentiment analysis JSON. Identify and extract key themes or topics discussed in the product review. Output the themes in a JSON array format.\\n\\nSentiment Analysis JSON: {sentiment_json}\\n\\nOutput your response in the following JSON format:\\n{\\n    \"themes\": [\"theme1\", \"theme2\", \"theme3\"]\\n}\\n'), ('extract_sentiment_score_prompt', '\\nAnalyze the following sentiment analysis JSON and extract the sentiment score. \\nOutput the sentiment score as a numeric value.\\n\\nSentiment Analysis JSON: {sentiment_json}\\n\\nOutput your response as a JSON object:\\n{\\n    \"sentiment_score\": score\\n}\\n'), ('extract_sentiment_score_prompt_numeric', '\\nAnalyze the following sentiment analysis JSON and extract the sentiment score. \\nOutput the sentiment score as a numeric value.\\n\\nSentiment Analysis JSON: {sentiment_json}\\n\\nOutput your response as a number {sentiment_score}\\n')]\n",
      "[('default', 'gpt-3.5-turbo', 4096), ('gpt-4o', 'gpt-4o', 128000), ('text-embedding-3-small', 'text-embedding-3-small', 0), ('semantic_analyzer', 'gpt-4o-mini', 128000)]\n"
     ]
    }
   ],
   "source": [
    "available_prompts = con.execute(\"get prompt;\").fetchall()\n",
    "available_models = con.execute(\"get model;\").fetchall()\n",
    "\n",
    "print(available_prompts)\n",
    "print(available_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's go through the tasks one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing Sentiment Analysis on Product Reviews:\n",
    "The first step is to analyze the sentiment of each review. We can use LLMs to generate a detailed sentiment analysis by combining the review text with the star rating.\n",
    "\n",
    "We use `lf_map`, a function that calls the `semantic analyzer` model to perform sentiment analysis on each review.\\\n",
    "`semantic analyzer` is an alias for `gpt-4o-mini` \\\n",
    "The model evaluates the sentiment based on both the review text and the star rating.\n",
    "This step gives us a sentiment score or label (e.g., positive, negative, neutral) for each review, providing a solid foundation for deeper analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb22700c31f41c48178d1d881092e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE sentiment_analysis AS\n",
    "WITH sentiment_analysis AS (\n",
    "    SELECT \n",
    "        ProductID as product_id, \n",
    "        ID AS review_id,\n",
    "        Review AS review_text, \n",
    "        Rating AS star_rating, \n",
    "        lf_map('sentiment_analysis_prompt', 'semantic_analyzer', {'review': review_text, 'rating': star_rating}) AS sentiment_json\n",
    "    FROM \n",
    "        product_reviews\n",
    ")\n",
    "SELECT * \n",
    "FROM sentiment_analysis;\n",
    "\"\"\"\n",
    "\n",
    "# Run the query and fetch results\n",
    "results = con.execute(query).fetchall()\n",
    "\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets take a look at the newly created sentiment_analysis table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 1, 'This smartphone is amazing! The camera quality is outstanding, and the battery life is impressive. Highly recommended!', 5, '{\"explanation\":\"The review expresses strong enthusiasm and satisfaction with the smartphone, highlighting its camera quality and battery life positively.\",\"rating\":\"5\",\"review\":\"This smartphone is amazing! The camera quality is outstanding, and the battery life is impressive. Highly recommended!\",\"sentiment\":\"positive\"}')\n",
      "(101, 2, \"Decent phone, but the battery drains too quickly. Otherwise, it's okay.\", 3, '{\"explanation\":\"The review is mixed, acknowledging some decent features while also pointing out a significant drawback, leading to a neutral sentiment overall.\",\"rating\":\"3\",\"review\":\"Decent phone, but the battery drains too quickly. Otherwise, it\\'s okay.\",\"sentiment\":\"neutral\"}')\n",
      "(102, 3, 'These headphones are a game-changer. The noise cancellation is top-notch, and the sound quality is superb.', 5, '{\"explanation\":\"The review conveys strong positive sentiment, praising the headphones for their noise cancellation and sound quality.\",\"rating\":\"5\",\"review\":\"These headphones are a game-changer. The noise cancellation is top-notch, and the sound quality is superb.\",\"sentiment\":\"positive\"}')\n",
      "(102, 4, 'Comfortable to wear, but the sound quality is just average. Expected better for the price.', 3, '{\"explanation\":\"The review indicates a balance of positive and negative aspects, resulting in a neutral sentiment due to the average sound quality.\",\"rating\":\"3\",\"review\":\"Comfortable to wear, but the sound quality is just average. Expected better for the price.\",\"sentiment\":\"neutral\"}')\n",
      "(103, 5, \"This laptop is a disappointment. It's slow, the build quality is poor, and it overheats easily. Avoid!\", 1, '{\"explanation\":\"The review expresses strong dissatisfaction with multiple negative aspects of the laptop, leading to a clear negative sentiment.\",\"rating\":\"1\",\"review\":\"This laptop is a disappointment. It\\'s slow, the build quality is poor, and it overheats easily. Avoid!\",\"sentiment\":\"negative\"}')\n",
      "(103, 6, \"Great laptop for everyday use. It's fast, lightweight, and has a good battery life. Very satisfied with my purchase.\", 4, '{\"explanation\":\"The review reflects a positive experience, highlighting the laptop\\'s strengths and the satisfaction of the user.\",\"rating\":\"4\",\"review\":\"Great laptop for everyday use. It\\'s fast, lightweight, and has a good battery life. Very satisfied with my purchase.\",\"sentiment\":\"positive\"}')\n",
      "(104, 7, 'This smartwatch is fantastic! It accurately tracks my workouts and the battery lasts for days. Love it!', 5, '{\"explanation\":\"The review shows strong enthusiasm and satisfaction with the smartwatch, particularly praising its tracking accuracy and battery life.\",\"rating\":\"5\",\"review\":\"This smartwatch is fantastic! It accurately tracks my workouts and the battery lasts for days. Love it!\",\"sentiment\":\"positive\"}')\n",
      "(104, 8, 'The smartwatch looks good, but the step counter seems inaccurate. Also, the screen is a bit too small for my liking.', 3, '{\"explanation\":\"The review presents both positive (appearance) and negative (functionality) aspects, resulting in a neutral sentiment.\",\"rating\":\"3\",\"review\":\"The smartwatch looks good, but the step counter seems inaccurate. Also, the screen is a bit too small for my liking.\",\"sentiment\":\"neutral\"}')\n",
      "(105, 9, 'Terrible product! The coffee maker leaked after just a week of use. Customer service was unhelpful. Never buying from this brand again.', 1, '{\"explanation\":\"The review expresses strong dissatisfaction with the product and customer service, clearly indicating a negative sentiment.\",\"rating\":\"1\",\"review\":\"Terrible product! The coffee maker leaked after just a week of use. Customer service was unhelpful. Never buying from this brand again.\",\"sentiment\":\"negative\"}')\n",
      "(105, 10, 'Makes great coffee and is easy to clean. The programmable timer is a nice feature. Good value for money.', 4, '{\"explanation\":\"The review positively highlights the features of the coffee maker, indicating satisfaction and a positive sentiment.\",\"rating\":\"4\",\"review\":\"Makes great coffee and is easy to clean. The programmable timer is a nice feature. Good value for money.\",\"sentiment\":\"positive\"}')\n"
     ]
    }
   ],
   "source": [
    "query = \"select * from sentiment_analysis;\"\n",
    "results = con.execute(query).fetchall()\n",
    "\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering for High-Impact Reviews\n",
    "Not all reviews are equally important. Some are more detailed, more critical, or offer particularly valuable feedback. In this next step, we filter out the \"high-impact\" reviews—those that deserve immediate attention.\n",
    "\n",
    "This query introduces lf_filter, which uses the current model to determine whether a review is \"high-impact\" based on the sentiment, rating, and length of the review. High-impact reviews could include:\n",
    "\n",
    "Negative reviews that are detailed and provide insights into product issues.\n",
    "Positive reviews that highlight key features or advantages (useful for marketing).\n",
    "By focusing on these high-impact reviews, you can address critical feedback and leverage valuable testimonials effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7,)\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE filtered_reviews AS\n",
    "WITH filtered_reviews AS (\n",
    "    SELECT \n",
    "        * \n",
    "    FROM \n",
    "        sentiment_analysis \n",
    "    WHERE \n",
    "        lf_filter('is_high_impact_review_prompt', 'gpt-4o', {\n",
    "            'sentiment': sentiment_json, \n",
    "            'rating': star_rating, \n",
    "            'review_length': LENGTH(review_text)\n",
    "        })\n",
    ")\n",
    "SELECT * \n",
    "FROM filtered_reviews;\n",
    "\"\"\"\n",
    "\n",
    "# Run the query and fetch results\n",
    "results = con.execute(query).fetchall()\n",
    "\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Key Themes from Reviews\n",
    "\n",
    "Once you’ve identified high-impact reviews, the next step is understanding why customers leave them.\n",
    "This query helps identify recurring themes such as complaints about battery life, praises for design, or issues with customer service.\n",
    "This query extracts common themes using LLM's ability to understand and categorize text. You’ll get insights like:\n",
    "\n",
    "Common Complaints: \"The battery drains too quickly.\"\n",
    "Positive Feedback: \"The screen resolution is excellent.\"\n",
    "\n",
    "We first define our own prompt called `extract_themes`, and then use it to extract the themes from `filtered_reviews`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConstraintException",
     "evalue": "Constraint Error: Duplicate key \"prompt_name: extract_themes_prompt\" violates primary key constraint. If this is an unexpected constraint violation please double check with the known index limitations section in our documentation (https://duckdb.org/docs/sql/indexes).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConstraintException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 22\u001b[0m\n\u001b[1;32m     17\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124mCREATE PROMPT (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextract_themes_prompt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mescaped_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m);\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Execute the query to create the prompt\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfetchall()\n",
      "\u001b[0;31mConstraintException\u001b[0m: Constraint Error: Duplicate key \"prompt_name: extract_themes_prompt\" violates primary key constraint. If this is an unexpected constraint violation please double check with the known index limitations section in our documentation (https://duckdb.org/docs/sql/indexes)."
     ]
    }
   ],
   "source": [
    "# Define the extract themes prompt\n",
    "extract_themes_prompt = \"\"\"\n",
    "Analyze the following sentiment analysis JSON. Identify and extract key themes or topics discussed in the product review. Output the themes in a JSON array format.\n",
    "\n",
    "Sentiment Analysis JSON: {sentiment_json}\n",
    "\n",
    "Output your response in the following JSON format:\n",
    "{\n",
    "    \"themes\": [\"theme1\", \"theme2\", \"theme3\"]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Escape single quotes in the prompt for SQL\n",
    "escaped_prompt = extract_themes_prompt.replace(\"'\", \"''\")\n",
    "\n",
    "# Create the SQL query to register the prompt\n",
    "query = f\"\"\"\n",
    "CREATE PROMPT ('extract_themes_prompt', '{escaped_prompt}');\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query to create the prompt\n",
    "results = con.execute(query).fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d1fd812cde4bbb9e5704d5c919d78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7,)\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE themes_extracted AS\n",
    "WITH themes_extracted AS (\n",
    "    SELECT \n",
    "        product_id, \n",
    "        review_id, \n",
    "        review_text,\n",
    "        star_rating,\n",
    "        lf_map('extract_themes_prompt', 'gpt-4o', {'sentiment_json': sentiment_json}) AS themes\n",
    "    FROM \n",
    "        filtered_reviews\n",
    ")\n",
    "SELECT * \n",
    "FROM themes_extracted;\n",
    "\"\"\"\n",
    "# Run the query and fetch results\n",
    "results = con.execute(query).fetchall()\n",
    "\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking Customer Sentiment Over Time\n",
    "Customer feedback changes as new product updates are rolled out or customer expectations evolve. To track customer satisfaction over time, you can analyze how the average sentiment and star ratings change month by month.\n",
    "\n",
    "We use 2 different prompts here `sentiment_analysis_prompt` and `extract_sentiment_score_prompt_numeric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x77b02c14eef0>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the extract sentiment score prompt\n",
    "extract_sentiment_score_prompt_numeric = \"\"\"\n",
    "Analyze the following sentiment analysis JSON and extract the sentiment score. \n",
    "Output the sentiment score as a numeric value.\n",
    "\n",
    "Sentiment Analysis JSON: {sentiment_json}\n",
    "\n",
    "Output your response as a number {sentiment_score}\n",
    "\"\"\"\n",
    "\n",
    "# Define the SQL query to create the prompt\n",
    "query = f\"\"\"\n",
    "CREATE PROMPT ('extract_sentiment_score_prompt_numeric', '{extract_sentiment_score_prompt_numeric}');\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query to create the prompt\n",
    "con.execute(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d62c01d86a24c3dba175b87b403343e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 2024, 3, 4.0, 4.0)\n",
      "(102, 2024, 3, 4.0, 4.0)\n",
      "(103, 2024, 3, 2.5, 2.5)\n",
      "(104, 2024, 3, 4.0, 4.0)\n",
      "(105, 2024, 3, 2.5, 2.5)\n"
     ]
    }
   ],
   "source": [
    "# Define the SQL query\n",
    "query = \"\"\"\n",
    "WITH sentiment_analysis_dated AS (\n",
    "    SELECT \n",
    "        ProductID as product_id, \n",
    "        ID AS review_id,\n",
    "        Review AS review_text, \n",
    "        Rating AS star_rating, \n",
    "        Date as review_date,\n",
    "        lf_map('sentiment_analysis_prompt', 'gpt-4o', {'review': review_text, 'rating': star_rating}) AS sentiment_json\n",
    "    FROM \n",
    "        product_reviews\n",
    ")\n",
    "SELECT \n",
    "    product_id, \n",
    "    EXTRACT(YEAR FROM review_date) AS year, \n",
    "    EXTRACT(MONTH FROM review_date) AS month, \n",
    "    AVG(star_rating) AS avg_star_rating, \n",
    "    AVG(\n",
    "        CAST(\n",
    "            (lf_map('extract_sentiment_score_prompt', 'gpt-4o', {'sentiment_json': sentiment_json}))->>'sentiment_score' AS DOUBLE\n",
    "        )\n",
    "    )\n",
    "     AS avg_sentiment_score\n",
    "FROM \n",
    "    sentiment_analysis_dated\n",
    "GROUP BY \n",
    "    product_id, \n",
    "    EXTRACT(YEAR FROM review_date), \n",
    "    EXTRACT(MONTH FROM review_date)\n",
    "ORDER BY \n",
    "    year DESC, month DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and fetch the results\n",
    "results = con.execute(query).fetchall()\n",
    "\n",
    "# Print the results\n",
    "for row in results:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And that is it, we have successfully created new prompts, used existing prompts and manipulated the standard table to gain analytical insights in our data, using the power of LLMs!\n",
    "\n",
    "We finally close the DuckDB connection. The data is persisted into out database `mydb.db`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the DuckDB connection\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x77b02c14eef0>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute (\"DROP TABLE filtered_reviews;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x77b02c14eef0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute (\"DROP TABLE sentiment_analysis ;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x77b02c14eef0>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute (\"delete prompt 'extract_sentiment_score_prompt_numeric';\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x77b02c14eef0>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute (\"delete prompt 'extract_themes_prompt';\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x77b02c14eef0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute (\"DROP TABLE themes_extracted ;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
